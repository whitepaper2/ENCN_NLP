{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['开通', '家庭', '网\\r\\n'], ['手机', '流量', '优惠', '总介\\r\\n'], ['二季度', '融合', '套餐', '提', '档', '优惠\\r\\n'], ['更改', '4', 'G', '自选', '套餐', '的', '生效', '规则\\r\\n'], ['4', 'G', '飞', '享', '套餐', '的', '资费', '介绍\\r\\n'], ['5', '块', '钱', '流量\\r\\n'], ['你', '脑子', '有', '问题', '吧\\r\\n'], ['你好\\r\\n'], ['4', 'G', '飞', '享', '套餐', '介绍\\r\\n'], ['飞', '享', '套餐', '5', '折', '优惠\\r\\n'], ['实名制', '的', '查询', '方法\\r\\n'], ['查询', '我', '的', '流量\\r\\n'], ['1\\r\\n'], ['领奖\\r\\n'], ['停机', '了', '，', '进不去\\r\\n'], ['17', '年', '流量', '年', '包', '优惠\\r\\n'], ['手机', '流量', '叠加', '包', '的', '总介\\r\\n'], ['0', '元', '抢', '3', 'G\\r\\n'], ['闲时', '流量', '是', '什么', '时候', '用', '的\\r\\n'], ['1\\r\\n'], ['查询', '流量', '？\\r\\n'], ['52163', '取消\\r\\n'], ['什么', '鬼\\r\\n'], ['对不起\\r\\n'], ['10086', '电话', '充', '值', '发票', '怎么', '领取\\r\\n'], ['代', '收费', '服务费\\r\\n'], ['晚安', '啦\\r\\n'], ['满意', '评', '10', '分\\r\\n'], ['移动', '宽带', '覆盖', '的', '查询', '方法\\r\\n'], ['我', '的', '10', 'G', '流量', '什么', '时候', '生\\r\\n'], ['彩', '铃', 'D', 'I', 'Y', '的', '开通', '方法\\r\\n'], ['赠送', '话费', '的', '赠送', '形式\\r\\n'], ['你', '干脆說', '你', '什么', '都', '不', '知道', '好', '了\\r\\n'], ['查', '流量\\r\\n'], ['闲时', '流量\\r\\n'], ['如何', '取消', '本机', '业务\\r\\n'], ['开通', '4', 'G', '自选', '套餐', '的', '生效', '规则\\r\\n'], ['P', 'I', 'N', '码', '介绍\\r\\n'], ['查询', '我', '的', '流量\\r\\n'], ['A', 'P', 'P', '充', '值', '9.5', '折', '优惠\\r\\n'], ['充值卡', '的', '介绍\\r\\n'], ['313\\r\\n'], ['19.99', '元', '20', 'G', '咪咕', '音乐', '特惠', '流量', '包\\r\\n'], ['办', '流量', '叠加', '包', '，', '再', '送', '1', 'G\\r\\n'], ['闲时\\r\\n'], ['宽带\\r\\n'], ['我', '这个', '号码', '是', '多少\\r\\n'], ['实名制', '修改', '方法\\r\\n'], ['1', '，', '4', 'G', '套餐', '取消', '方法\\r\\n'], ['第二', '季度', '套餐', '升档', '优惠\\r\\n'], ['A', 'P', 'P', '充', '值', '9.5', '折', '优惠\\r\\n'], ['已', '开通', '业务\\r\\n'], ['查询', '我', '的', '流量\\r\\n'], ['实名制', '的', '查询', '方法\\r\\n'], ['我', '的', '流量', '什么', '时候', '到期\\r\\n'], ['为什么', '现在', '我', '的', '号码', '收', '不', '到', '验证', '码', '？\\r\\n'], ['恢复', '上网', '功能\\r\\n'], ['手机', '流量', '月', '套餐', '的', '总介\\r\\n'], ['0', '元', '1', 'G', '省内', '体验', '包', '优惠\\r\\n'], ['话费', '明细\\r\\n'], ['国内', '流量', '套餐', '推荐\\r\\n'], ['请问', '，', '本机', '有', '那些', '业务\\r\\n'], ['4\\r\\n'], ['18', '元', '4', 'G', '飞', '享', '套餐', '介绍\\r\\n'], ['逢', '周三', '免费', '领', '2万', '份', '流量\\r\\n'], ['我', '要', '退', '了', '短号\\r\\n'], ['8', '元', '4', 'G', '飞', '享', '套餐', '（', '省内', '）', '介绍\\r\\n'], ['4', 'G', '飞', '享', '套餐', '的', '资费', '介绍\\r\\n'], ['转', '38', '套餐', '送', '114', '元', '话费\\r\\n'], ['查询', '我', '的', '积分\\r\\n'], ['查询', '我', '的', '流量\\r\\n'], ['你好\\r\\n'], ['查询', '我', '的', '余额\\r\\n'], ['有', '哪', '几', '种', '充值卡', '金额', '?\\r\\n'], ['夜间\\r\\n'], ['查询', 'P', 'I', 'N\\r\\n'], ['宽带', '专区\\r\\n'], ['流量', '特惠', '包', '开通', '方法\\r\\n'], ['查询', '我', '的', '短号\\r\\n'], ['2017年', '流量', '年', '包', '优惠', '活动\\r\\n'], ['我', '爱', '我', '家', '的', '优惠', '话费', '什么', '时候', '返还', '？\\r\\n'], ['4', 'G', '飞', '享', '套餐', '的', '资费', '介绍\\r\\n'], ['苹果', '5', '怎样', '用', '4', 'G\\r\\n'], ['4\\r\\n'], ['为什么', '我', '不', '能', '取消', '流量', '套餐\\r\\n'], ['手机', '摇', '一', '摇', '，', '好', '礼', '你', '想', '要\\r\\n'], ['充值卡', '在', '充', '值', '时', '要', '收费', '吗', '?', '话费\\r\\n'], ['我', '的', '套餐\\r\\n'], ['我', '爱', '我家', '优惠', '什么', '时候', '到期\\r\\n'], ['办理', '了', '6元', '/', '月', '1', 'g', '流量\\r\\n'], ['8', '元', '4', 'G', '飞', '享', '套餐', '（', '省内', '）', '介绍\\r\\n'], ['流量', '特惠', '包', '的', '总介\\r\\n'], ['A\\r\\n'], ['可', '以\\r\\n'], ['办', '流量', '叠加', '包', '，', '再', '送', '1', 'G\\r\\n'], ['上个月', '7', '号', '充', '值', '50', '元', '，', '到', '现在', '都', '没', '赠送', '20\\r\\n'], ['#\\r\\n'], ['新人', '领', '300', 'M\\r\\n'], ['万能', '副', '卡', '0', '元', '领', '，', '10086', '办', '更', '送', '30', '元', '话费', '和', '6', 'G', '流量\\r\\n']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "\n",
    "segmenter = StanfordSegmenter(path_to_jar=\"./stanford-segmenter/stanford-segmenter-3.4.1.jar\", path_to_sihan_corpora_dict=\"./stanford-segmenter/data\", path_to_model=\"./stanford-segmenter/data/pku.gz\", path_to_dict=\"./stanford-segmenter/data/dict-chris6.ser.gz\",path_to_slf4j=\"./stanford-segmenter/slf4j-api.jar\")\n",
    "sent = u'这是斯坦福中文分词器测试'\n",
    "#print(segmenter.segment(sent))\n",
    "filename = \"./data/5月4日自动问答明细.xlsx\"\n",
    "#cols = [\"用户ID\",\"用户问题\"]\n",
    "#df = pd.read_excel(filename)\n",
    "#df_selected = df[cols]\n",
    "#print(df[cols].head(2))\n",
    "\n",
    "#df_segment = segmenter.segment(df_selected[\"用户问题\"].head(1))\n",
    "#print(df_segment)\n",
    "import xlrd\n",
    "xls_data = xlrd.open_workbook(filename)\n",
    "table = xls_data.sheets()[0]\n",
    "nrows = table.nrows #行数\n",
    "ncols = table.ncols #列数\n",
    "list2 =[]\n",
    "for rownum in range(1,100):\n",
    "    row = table.row_values(rownum)\n",
    "    if row:\n",
    "        list2.append(segmenter.segment(row[2]).split(\" \"))\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "newFreqSet= {'业务'} 377\n",
      "newFreqSet= {'业务', '介绍'} 145\n",
      "newFreqSet= {'业务', '套餐'} 21\n",
      "newFreqSet= {'业务', '介绍', '套餐'} 8\n",
      "newFreqSet= {'业务', '套餐', '流量'} 4\n",
      "newFreqSet= {'业务', '介绍', '套餐', '流量'} 3\n",
      "newFreqSet= {'方法', '业务'} 69\n",
      "newFreqSet= {'业务', '流量'} 32\n",
      "newFreqSet= {'业务', '介绍', '流量'} 13\n",
      "newFreqSet= {'方法', '业务', '流量'} 4\n",
      "newFreqSet= {'介绍'} 817\n",
      "newFreqSet= {'取消'} 307\n",
      "newFreqSet= {'业务', '取消'} 35\n",
      "newFreqSet= {'方法', '业务', '取消'} 25\n",
      "newFreqSet= {'取消', '套餐'} 66\n",
      "newFreqSet= {'方法', '取消', '套餐'} 48\n",
      "newFreqSet= {'取消', '开通'} 3\n",
      "newFreqSet= {'取消', '手机'} 29\n",
      "newFreqSet= {'取消', '套餐', '手机'} 6\n",
      "newFreqSet= {'方法', '取消', '套餐', '手机'} 3\n",
      "newFreqSet= {'手机', '取消', '套餐', '方法', '流量'} 3\n",
      "newFreqSet= {'取消', '套餐', '手机', '流量'} 6\n",
      "newFreqSet= {'方法', '取消', '手机'} 24\n",
      "newFreqSet= {'取消', '手机', '流量'} 10\n",
      "newFreqSet= {'方法', '取消', '手机', '流量'} 7\n",
      "newFreqSet= {'方法', '取消'} 219\n",
      "newFreqSet= {'取消', '流量'} 31\n",
      "newFreqSet= {'取消', '套餐', '流量'} 15\n",
      "newFreqSet= {'方法', '取消', '套餐', '流量'} 12\n",
      "newFreqSet= {'方法', '取消', '流量'} 28\n",
      "newFreqSet= {'资费', '取消'} 4\n",
      "newFreqSet= {'方法', '资费', '取消'} 3\n",
      "newFreqSet= {'套餐'} 591\n",
      "newFreqSet= {'介绍', '套餐'} 164\n",
      "newFreqSet= {'方法', '套餐'} 118\n",
      "newFreqSet= {'开通'} 319\n",
      "newFreqSet= {'业务', '开通'} 32\n",
      "newFreqSet= {'业务', '手机', '开通'} 5\n",
      "newFreqSet= {'方法', '业务', '手机', '开通'} 4\n",
      "newFreqSet= {'方法', '业务', '开通'} 23\n",
      "newFreqSet= {'业务', '流量', '开通'} 3\n",
      "newFreqSet= {'套餐', '开通'} 75\n",
      "newFreqSet= {'方法', '套餐', '开通'} 47\n",
      "newFreqSet= {'手机', '开通'} 40\n",
      "newFreqSet= {'套餐', '手机', '开通'} 14\n",
      "newFreqSet= {'方法', '套餐', '手机', '开通'} 11\n",
      "newFreqSet= {'手机', '套餐', '开通', '方法', '流量'} 11\n",
      "newFreqSet= {'套餐', '手机', '流量', '开通'} 14\n",
      "newFreqSet= {'方法', '手机', '开通'} 33\n",
      "newFreqSet= {'手机', '流量', '开通'} 23\n",
      "newFreqSet= {'方法', '手机', '流量', '开通'} 20\n",
      "newFreqSet= {'方法', '开通'} 216\n",
      "newFreqSet= {'流量', '开通'} 66\n",
      "newFreqSet= {'套餐', '流量', '开通'} 26\n",
      "newFreqSet= {'方法', '套餐', '流量', '开通'} 19\n",
      "newFreqSet= {'方法', '流量', '开通'} 52\n",
      "newFreqSet= {'资费', '开通'} 8\n",
      "newFreqSet= {'方法', '资费', '开通'} 6\n",
      "newFreqSet= {'手机'} 341\n",
      "newFreqSet= {'业务', '手机'} 37\n",
      "newFreqSet= {'业务', '介绍', '手机'} 15\n",
      "newFreqSet= {'方法', '业务', '手机'} 8\n",
      "newFreqSet= {'介绍', '手机'} 81\n",
      "newFreqSet= {'介绍', '手机', '流量'} 31\n",
      "newFreqSet= {'套餐', '手机'} 60\n",
      "newFreqSet= {'介绍', '套餐', '手机'} 22\n",
      "newFreqSet= {'介绍', '套餐', '手机', '流量'} 22\n",
      "newFreqSet= {'方法', '套餐', '手机'} 14\n",
      "newFreqSet= {'方法', '套餐', '手机', '流量'} 14\n",
      "newFreqSet= {'套餐', '手机', '流量'} 60\n",
      "newFreqSet= {'方法', '手机'} 90\n",
      "newFreqSet= {'方法', '手机', '流量'} 30\n",
      "newFreqSet= {'手机', '流量'} 121\n",
      "newFreqSet= {'方法'} 769\n",
      "newFreqSet= {'方法', '介绍'} 5\n",
      "newFreqSet= {'流量'} 482\n",
      "newFreqSet= {'介绍', '流量'} 97\n",
      "newFreqSet= {'介绍', '套餐', '流量'} 41\n",
      "newFreqSet= {'套餐', '流量'} 165\n",
      "newFreqSet= {'方法', '流量'} 94\n",
      "newFreqSet= {'方法', '套餐', '流量'} 34\n",
      "newFreqSet= {'资费'} 329\n",
      "newFreqSet= {'资费', '业务'} 29\n",
      "newFreqSet= {'资费', '介绍', '业务'} 16\n",
      "newFreqSet= {'资费', '业务', '手机'} 4\n",
      "newFreqSet= {'资费', '业务', '流量'} 3\n",
      "newFreqSet= {'资费', '介绍'} 169\n",
      "newFreqSet= {'资费', '套餐'} 66\n",
      "newFreqSet= {'资费', '介绍', '套餐'} 45\n",
      "newFreqSet= {'资费', '套餐', '流量'} 25\n",
      "newFreqSet= {'资费', '介绍', '套餐', '流量'} 20\n",
      "newFreqSet= {'资费', '手机'} 36\n",
      "newFreqSet= {'资费', '介绍', '手机'} 25\n",
      "newFreqSet= {'资费', '套餐', '手机'} 12\n",
      "newFreqSet= {'资费', '介绍', '套餐', '手机'} 12\n",
      "newFreqSet= {'资费', '套餐', '手机', '流量'} 12\n",
      "newFreqSet= {'资费', '介绍', '手机', '套餐', '流量'} 12\n",
      "newFreqSet= {'资费', '手机', '流量'} 24\n",
      "newFreqSet= {'资费', '介绍', '手机', '流量'} 19\n",
      "newFreqSet= {'方法', '资费'} 9\n",
      "newFreqSet= {'资费', '流量'} 75\n",
      "newFreqSet= {'资费', '介绍', '流量'} 52\n",
      "freq: [{'业务'}, {'业务', '介绍'}, {'业务', '套餐'}, {'业务', '介绍', '套餐'}, {'业务', '套餐', '流量'}, {'业务', '介绍', '套餐', '流量'}, {'方法', '业务'}, {'业务', '流量'}, {'业务', '介绍', '流量'}, {'方法', '业务', '流量'}, {'介绍'}, {'取消'}, {'业务', '取消'}, {'方法', '业务', '取消'}, {'取消', '套餐'}, {'方法', '取消', '套餐'}, {'取消', '开通'}, {'取消', '手机'}, {'取消', '套餐', '手机'}, {'方法', '取消', '套餐', '手机'}, {'手机', '取消', '套餐', '方法', '流量'}, {'取消', '套餐', '手机', '流量'}, {'方法', '取消', '手机'}, {'取消', '手机', '流量'}, {'方法', '取消', '手机', '流量'}, {'方法', '取消'}, {'取消', '流量'}, {'取消', '套餐', '流量'}, {'方法', '取消', '套餐', '流量'}, {'方法', '取消', '流量'}, {'资费', '取消'}, {'方法', '资费', '取消'}, {'套餐'}, {'介绍', '套餐'}, {'方法', '套餐'}, {'开通'}, {'业务', '开通'}, {'业务', '手机', '开通'}, {'方法', '业务', '手机', '开通'}, {'方法', '业务', '开通'}, {'业务', '流量', '开通'}, {'套餐', '开通'}, {'方法', '套餐', '开通'}, {'手机', '开通'}, {'套餐', '手机', '开通'}, {'方法', '套餐', '手机', '开通'}, {'手机', '套餐', '开通', '方法', '流量'}, {'套餐', '手机', '流量', '开通'}, {'方法', '手机', '开通'}, {'手机', '流量', '开通'}, {'方法', '手机', '流量', '开通'}, {'方法', '开通'}, {'流量', '开通'}, {'套餐', '流量', '开通'}, {'方法', '套餐', '流量', '开通'}, {'方法', '流量', '开通'}, {'资费', '开通'}, {'方法', '资费', '开通'}, {'手机'}, {'业务', '手机'}, {'业务', '介绍', '手机'}, {'方法', '业务', '手机'}, {'介绍', '手机'}, {'介绍', '手机', '流量'}, {'套餐', '手机'}, {'介绍', '套餐', '手机'}, {'介绍', '套餐', '手机', '流量'}, {'方法', '套餐', '手机'}, {'方法', '套餐', '手机', '流量'}, {'套餐', '手机', '流量'}, {'方法', '手机'}, {'方法', '手机', '流量'}, {'手机', '流量'}, {'方法'}, {'方法', '介绍'}, {'流量'}, {'介绍', '流量'}, {'介绍', '套餐', '流量'}, {'套餐', '流量'}, {'方法', '流量'}, {'方法', '套餐', '流量'}, {'资费'}, {'资费', '业务'}, {'资费', '介绍', '业务'}, {'资费', '业务', '手机'}, {'资费', '业务', '流量'}, {'资费', '介绍'}, {'资费', '套餐'}, {'资费', '介绍', '套餐'}, {'资费', '套餐', '流量'}, {'资费', '介绍', '套餐', '流量'}, {'资费', '手机'}, {'资费', '介绍', '手机'}, {'资费', '套餐', '手机'}, {'资费', '介绍', '套餐', '手机'}, {'资费', '套餐', '手机', '流量'}, {'资费', '介绍', '手机', '套餐', '流量'}, {'资费', '手机', '流量'}, {'资费', '介绍', '手机', '流量'}, {'方法', '资费'}, {'资费', '流量'}, {'资费', '介绍', '流量'}]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('D:/code/python/kaggle/chineseNlp')\n",
    "import fpGrowth\n",
    "from fpGrowth import *\n",
    "initSet = createInitSet(list4)\n",
    "#print(initSet)\n",
    "myFPtree, myHeaderTab = createTree(initSet, 300)\n",
    "# # 创建条件模式基\n",
    "freqItemList = []\n",
    "\n",
    "mineTree(myFPtree, myHeaderTab, 3, set([]), freqItemList)\n",
    "print(\"freq:\",freqItemList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'流量': [482, <fpGrowth.treeNode object at 0x000000000BFC8198>], '资费': [329, <fpGrowth.treeNode object at 0x000000000BFC8518>], '手机': [341, <fpGrowth.treeNode object at 0x000000000BFC8048>], '套餐': [591, <fpGrowth.treeNode object at 0x000000000BFC84E0>], '业务': [377, <fpGrowth.treeNode object at 0x000000000BFC8550>], '开通': [319, <fpGrowth.treeNode object at 0x0000000022230D68>], '方法': [769, <fpGrowth.treeNode object at 0x0000000022230358>], '取消': [307, <fpGrowth.treeNode object at 0x00000000222300F0>], '介绍': [817, <fpGrowth.treeNode object at 0x0000000022230390>]}\n"
     ]
    }
   ],
   "source": [
    "#print(len(list4))\n",
    "print(myHeaderTab)\n",
    "import skflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '来到', '北京', '清华大学']\n",
      "[全模式]:  \n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "text = \"我来到北京清华大学\"\n",
    "seg_list = jieba.cut(text, cut_all=False)\n",
    "list3 = []\n",
    "for i in seg_list:\n",
    "    list3.append(i)\n",
    "print(list3)\n",
    "print(u\"[全模式]: \", \",\".join(seg_list))\n",
    "# np.savetxt(\"./data/segwords.txt\", list3)\n",
    "file=open('./data/data.txt','a')  \n",
    "file.write(str(list3)+\"\\n\");  \n",
    "file.close()  \n",
    "import xlrd\n",
    "filename = \"./data/4月28日自动问答明细 (1).xlsx\"\n",
    "xls_data = xlrd.open_workbook(filename)\n",
    "table = xls_data.sheets()[0]\n",
    "nrows = table.nrows #行数\n",
    "ncols = table.ncols #列数\n",
    "list4 =[]\n",
    "english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%', '，',' ','？','/','！','（','）','“','”','：','-']\n",
    "stop_words = [ line.rstrip() for line in open('./data/stopwords.txt') ]\n",
    "file=open('./data/segwords.txt','a') \n",
    "for rownum in range(1,nrows):\n",
    "    row = table.row_values(rownum)\n",
    "    if row:\n",
    "        app = [ i for i in jieba.cut(row[3],cut_all=False) if i not in stop_words and i not in english_punctuations ]\n",
    "        if len(app)>=1:\n",
    "            list4.append(app)  \n",
    "            file.write(str(app)+\"\\n\");          \n",
    "        \n",
    "# print(list4)\n",
    "# file.write(str(list4));  \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\pengyuan.li/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'D:\\\\devInstallPath\\\\Anaconda3\\\\nltk_data'\n    - 'D:\\\\devInstallPath\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\pengyuan.li\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3ebad0ccb721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"And now for something compeletely differently\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\devInstallPath\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    110\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\devInstallPath\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\devInstallPath\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\devInstallPath\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\devInstallPath\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\pengyuan.li/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'D:\\\\devInstallPath\\\\Anaconda3\\\\nltk_data'\n    - 'D:\\\\devInstallPath\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\pengyuan.li\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = nltk.word_tokenize(\"And now for something compeletely differently\")\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}